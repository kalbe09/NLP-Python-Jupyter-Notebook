{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bereinigung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, warnings\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import pandas as pd\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')  # Let's not pay heed to them right now\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bereinigungsfunktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text to lower case\n",
    "def tolower(doc):\n",
    "    doc = [token.lower_ for token in doc]\n",
    "    doc = ' '.join(doc)\n",
    "    return nlp.make_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizer\n",
    "# Bring words in a normal form\n",
    "def lemmatizer(doc):\n",
    "    doc = [token.lemma_ for token in doc if token.lemma_ != 'PRON']\n",
    "    doc = u' '.join(doc)\n",
    "    return nlp.make_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes all numbers without letters\n",
    "def notNUM(doc):\n",
    "    doc = [token.text for token in doc if token.pos_ != 'NUM']\n",
    "    doc = u' '.join(doc)\n",
    "    return nlp.make_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes all symbols\n",
    "def noSYM(doc):\n",
    "    doc = [token.text.lower() for token in doc if token.pos_ != 'SYM']\n",
    "    doc = u' '.join(doc)\n",
    "    return nlp.make_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes single characters\n",
    "def single_char(doc):\n",
    "    doc = [token.text for token in doc if len(token.text) > 2]\n",
    "    doc = u' '.join(doc)\n",
    "    return nlp.make_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes punctuation\n",
    "def rm_punct(doc):\n",
    "    doc = [token.text for token in doc if not token.is_punct]\n",
    "    doc = u' '.join(doc)\n",
    "    return nlp.make_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes adjectives\n",
    "def rm_adj(doc):\n",
    "    doc = [token.text for token in doc if token.pos_ != 'ADJ']\n",
    "    doc = u' '.join(doc)\n",
    "    return nlp.make_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes other not identifable word\n",
    "def rm_other(doc):\n",
    "    doc = [token.text for token in doc if token.pos_ != 'X']\n",
    "    doc = u' '.join(doc)\n",
    "    return nlp.make_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes verbs\n",
    "def rm_verb(doc):\n",
    "    doc = [token.text for token in doc if token.pos_ != 'VERB']\n",
    "    doc = u' '.join(doc)\n",
    "    return nlp.make_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_additional_patterns(doc):\n",
    "    #print(doc)\n",
    "    doc = re.sub('ã*', '', doc)\n",
    "    doc = re.sub('\\n', '', doc)\n",
    "    doc = re.sub('â*', '', doc)\n",
    "    doc = re.sub('ã*', '', doc)\n",
    "    doc = re.sub('ë*', '', doc)\n",
    "    doc = re.sub('\\d*', '', doc)\n",
    "    doc = re.sub('¢*', '', doc)\n",
    "    #print(doc)\n",
    "    #doc = re.sub('*Ã£â€šÃ¢', '', doc)\n",
    "    #print(doc)\n",
    "    #doc = re.sub('*ã‚â', '', doc)\n",
    "    #print(doc)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updates spacy's default stop words list with my additional words\n",
    "def add_own_stopwords(stop_list=[]):\n",
    "    nlp.Defaults.stop_words.update(stop_list)\n",
    "\n",
    "    # Iterates over the words in the stop words list and resets the \"is_stop\" flag.\n",
    "    for word in STOP_WORDS:\n",
    "        lexeme = nlp.vocab[word]\n",
    "        lexeme.is_stop = True\n",
    "\n",
    "\n",
    "def remove_stopwords(doc):\n",
    "    # This will remove stopwords and punctuation.\n",
    "    # Use token.text to return strings, which we'll need for Gensim.\n",
    "    doc = [token.text for token in doc if token.is_stop != True and token.is_punct != True and token.is_space != True]\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pipline(my_stop_words):\n",
    "    try:\n",
    "        add_own_stopwords(my_stop_words)\n",
    "        nlp.add_pipe(tolower, name='tolower', first=True)\n",
    "        nlp.add_pipe(lemmatizer, name='lemmatizer', after='ner')\n",
    "        nlp.add_pipe(rm_other, name='rm_other', after='ner')\n",
    "        nlp.add_pipe(notNUM, name='notNUM', after='ner')\n",
    "        nlp.add_pipe(single_char, name='single_char', after='ner')\n",
    "        #nlp.add_pipe(remove_additional_patterns, name='rmpat', after='ner')\n",
    "        \n",
    "        #nlp.add_pipe(rm_adj, name='rm_adj', after='ner')\n",
    "        #nlp.add_pipe(rm_verb,name='rm_verb',after='ner')\n",
    "        \n",
    "        # Bringen keine Ergebnisse\n",
    "            #nlp.add_pipe(noSYM, name='noSYM', after='ner')\n",
    "            #nlp.add_pipe(rm_punct, name='rm_punct', after='ner')\n",
    "        \n",
    "        nlp.add_pipe(remove_stopwords, name=\"stopwords\", last=True)\n",
    "        #nlp.to_pickle(\"data/nlp.pkl\")\n",
    "    except ValueError as err:\n",
    "        print(err)\n",
    "    return nlp\n",
    "\n",
    "my_stop_words = ['\\ufeff1', '¢‚¬*', 'ë*']\n",
    "nlp = to_pipline(my_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anwendung auf eine Spalte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "def clean_column(column, filename):\n",
    "    indizes = column.index.tolist()\n",
    "    index = 0\n",
    "    for text in column:\n",
    "        pr = nlp(text)\n",
    "        strpr = u' '.join(pr)\n",
    "        strpr = remove_additional_patterns(strpr)\n",
    "        column.loc[indizes[index]] = strpr\n",
    "        index += 1\n",
    "    column.to_pickle(\"data/\"+ filename +\".pkl\")\n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "# ****************************************************************************\n",
    "# Define own Stop Words\n",
    "my_stop_words = ['\\ufeff1', '¢‚¬*', 'ë*']\n",
    "\n",
    "\n",
    "# Laden des Corpus\n",
    "file = open(\"data/knowledgebase.pkl\", \"rb\")\n",
    "kb = pickle.load(file)\n",
    "\n",
    "# Ausführen der Bereinigung\n",
    "kb['Topic'] = clean_column(kb['Topic'], 'clean_Topic')\n",
    "kb['Question'] = clean_column(kb['Question'], 'clean_Question')\n",
    "kb['Reference'] = clean_column(kb['Reference'], 'clean_Reference')\n",
    "kb['Enviroment'] = clean_column(kb['Enviroment'], 'clean_Enviroment')\n",
    "kb['Resolution'] = clean_column(kb['Resolution'], 'clean_Resolution')\n",
    "kb.to_csv(\"r/kb.csv\", sep=\";\")\n",
    "kb.to_pickle(\"data/clean_kb.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
